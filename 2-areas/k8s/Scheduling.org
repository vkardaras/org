#+title: Scheduling

* Manual Scheduling

Define node in pod

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  nodeName: node02
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 8080
#+end_src

** Reassigning a Running Pod Using a Binding Object

#+begin_src yaml
apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node02
#+end_src
* Labels and Selectors

** Selectors

Get pods with specific label

#+begin_src sh

# get pods with the label app=App1
kubectl get pods --selector app=App1

# filter pods with multiple labels
kubectl get pods --selector bu=finance,env=prod,tier=frontend

#+end_src

Use the selector field in the ReplicaSet specification to match the labels defined on the Pods

#+begin_src yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp
#+end_src

** Annotations

Use annotations to store additional metadata that is not intended for selection

#+begin_src yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
  annotations:
    buildversion: "1.34"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp
#+end_src
* Taints and Tolerations

** Tainting a Node

Use the following command where you specify the node name along with a taint in the format of a key-value pair and taint effect
=kubectl taint nodes node-name key=value:taint-effect=

#+begin_src sh

# taint node one so that it only accepts pods associated with the application blue
kubectl taint nodes node1 app=blue:NoSchedule

#+end_src

*Taint effects*

- NoSchedule: Pods that do not tolerate the taint will not be scheduled on the node.
- PreferNoSchedule: The scheduler tries to avoid placing non-tolerating pods on the node, but it is not strictly enforced.
- NoExecute: New pods without a matching toleration will not be scheduled, and existing pods will be evicted from the node.

** Adding Tolerations to Pods

This toleration lets the pod bypass the taint and be scheduled on node one.

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: nginx-container
      image: nginx
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"
#+end_src

Get taints for the master node

#+begin_src sh
kubectl describe node kubemaster | grep Taint
#+end_src

** Node Selectors

Labeling a node

#+begin_src shell

# label node node1 with label size=Large
kubectl label nodes node-1 size=Large

#+end_src

Configure node selectors
Use =nodeSelector= to label a node

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  nodeSelector:
    size: Large
#+end_src

** Node Affinity

Node affinity allows more expressive rules than node selectors

Example of node affinity
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: size
                operator: In
                values:
                  - Large
#+end_src

*Node affinity operators*

- In
- NotIn
- Exists

*Scheduling behaviors for node affinity*

- requiredDuringSchedulingIgnoredDuringExecution: Required During Scheduling, Ignored During Execution
  The pod is scheduled only on nodes that fully satisfy the affinity rules.
  Once running, changes to node labels do not impact the pod.

- preferredDuringSchedulingIgnoredDuringExecution: Required During Scheduling, Ignored During Execution
  The scheduler prefers nodes that meet the affinity rules but will place the pod on another node if no matching nodes are available.
* Resource Limits
** Resource Requests

Define the minimum guaranteed CPU and memory when a container scheduled.

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    resources:
      requests:
        memory: "4Gi"
        cpu: 2
#+end_src

** Resource Limits

Restrict resources by defining resource limits

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    resources:
      requests:
        memory: "1Gi"
        cpu: 1
      limits:
        memory: "2Gi"
        cpu: 2
#+end_src

** Limit Ranges

LimitRanges are namespace-level objects that automatically assign default resource values to containers that do not specify them

#+begin_src yaml
# limit-range-cpu.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
    - default:
        cpu: 500m
      defaultRequest:
        cpu: 500m
      max:
        cpu: "1"
      min:
        cpu: 100m
      type: Container
#+end_src

#+begin_src yaml
# limit-range-memory.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
    - default:
        memory: 1Gi
      defaultRequest:
        memory: 1Gi
      max:
        memory: 1Gi
      min:
        memory: 500Mi
      type: Container
#+end_src
* Daemon Sets
** Use Cases for DaemonSets

DaemonSets are particularly useful in scenarios where you need to run background services or agents on every node. Some common use cases include:

- *Monitoring agents and log collectors:*
  Deploy monitoring tools or log collectors across every node to ensure comprehensive cluster-wide visibility without manual intervention.
- *Essential Kubernetes components:*
  Deploy critical components, such as kube-proxy, which Kubernetes requires on all worker nodes.
- *Networking solutions:*
  Ensure consistent deployment of networking agents like those used in VNet or weave-net across all nodes.

** Creating a DaemonSet

Creating a DaemonSet is analogous to creating a ReplicaSet.

#+begin_src yaml
# daemon-set-definition.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
    matchLabels:
      app: monitoring-agent
  template:
    metadata:
      labels:
        app: monitoring-agent
    spec:
      containers:
        - name: monitoring-agent
          image: monitoring-agent
#+end_src

** DaemonSet commands

#+begin_src shell

# create a daemonSet
kubectl create -f daemon-set-definition.yaml

# get daemonSets
kubectl get daemonsets

# get details of daemonSet
kubectl describe daemonset monitoring-daemon

#+end_src
* Priority Classes

Priority classes allow you to assign a numerical value to Pods, where a higher number indicates higher priority. For user-deployed applications, the value can range from approximately -2 billion to +1 billion.

#+begin_src shell

# get priority classes
kubectl get priorityclass

#+end_src

** Creating a New Priority Class

#+begin_src yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000000
description: "Priority class for mission critical pods"
#+end_src

Make PriorityClass the default for all pods

#+begin_src yaml
# priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000000
description: "Priority class for mission critical pods"
globalDefault: true
#+end_src

** Apply PriorityClass to a pod

#+begin_src yaml
# pod-definition.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 8080
  priorityClassName: high-priority
#+end_src

** Pod Priority and Preemption

preemptionPolicy types

- PreemptLowerPriority: the scheduler will evict lower priority Pods to free up resources for higher priority ones (default value)
- Never: ensures the Pod remains in the scheduling queue without evicting any existing Pods

Examples

#+begin_src yaml
# PreemptLowerPriority
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000000
description: "Priority class for mission critical pods"
preemptionPolicy: PreemptLowerPriority

# Never
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000000
description: "Priority class for mission critical pods"
preemptionPolicy: Never
#+end_src
* Admission Controllers

Admission controllers provide additional security to RBAC like

- Validating pod specifications (e.g., ensuring that images are not from a public Docker Hub registry or enforcing the prohibition of the "latest" tag).
- Rejecting pods running containers as the root user, or enforcing specific Linux capabilities.
- Ensuring required metadata like labels is included.

** Built-In Admission Controllers

Some of the built-in admission controllers in Kubernetes include:

- *AlwaysPullImages*: Forces image pulling on each pod creation.
- *DefaultStorageClass*: Automatically assigns a default storage class to PVCs if none is specified.
- *EventRateLimit*: Limits the number of concurrent API server requests to prevent overload.
- *NamespaceExists*: Rejects requests to operate in non-existent namespaces.


** Configuring Admission Controllers

To add a new admission controller, update the enable-admission-plugins flag on the Kube API server service. For a kubeadm-based setup, modify the kube-apiserver manifest file.

Traditional Kube API Server Service
#+begin_src shell
ExecStart=/usr/local/bin/kube-apiserver \
    --advertise-address=${INTERNAL_IP} \
    --allow-privileged=true \
    --apiserver-count=3 \
    --authorization-mode=Node,RBAC \
    --bind-address=0.0.0.0 \
    --enable-swagger-ui=true \
    --etcd-servers=https://127.0.0.1:2379 \
    --event-ttl=1h \
    --runtime-config=api/all \
    --service-cluster-ip-range=10.32.0.0/24 \
    --service-node-port-range=30000-32767 \
    --v=2 \
    --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
#+end_src

Kubeadm-Based Setup (API Server as a Pod)
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
    - --advertise-address=172.17.0.107
    - --allow-privileged=true
    - --enable-bootstrap-token-auth=true
    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
    name: kube-apiserver
#+end_src
