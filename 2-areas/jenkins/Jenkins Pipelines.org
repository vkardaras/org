#+title: Jenkins Pipelines

* Pipeline and Jenkinsfile

** Example Jenkinsfile

Below is an example Jenkinsfile that outlines four primary stages: Build, Unit Test, Dockerize, and Deploy.

#+begin_src shell
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                sh 'mvn build'
            }
        }
        stage('Unit Test') {
            steps {
                sh 'mvn test'
            }
        }
        stage('Dockerize') {
            steps {
                sh 'docker build -t imageName:version .'
                sh 'docker push imageName:version'
            }
        }
        stage('Deploy') {
            steps {
                sh 'kubectl apply -f deployment.yaml'
            }
        }
    }
}
#+end_src

** Declarative vs. Scripted Pipelines

Jenkins offers two primary types of pipelines:

- =Declarative Pipeline=: Uses a simplified and human-readable syntax that is easy to learnâ€”ideal for straightforward and well-defined workflows.
- =Scripted Pipeline=: Allows for full Groovy scripting, providing greater control and flexibility for complex logic, although it has a steeper learning curve.

** Benefits of Using Jenkins Pipelines
Jenkins pipelines enable a more streamlined CI/CD process with key advantages such as:

- Code-as-configuration for easy version control and sharing.
- Resilience with features like pause and resume.
- Efficient handling of complex build processes with minimal job maintenance.
- Seamless integration with numerous Jenkins plugins that extend pipeline capabilities.
* Additional Pipeline Configuration

Common stages include:

- *Source* Code Management: Checking out code from version control systems (e.g., GitHub, Bitbucket, GIT).
- *Build*: Compiling, building, and packaging your application.
- *Test*: Running test cases (unit, integration, etc.) to ensure quality.
- *Deploy*: Pushing build artifacts to staging or production environments.

The =pipeline= keyword denotes the start of your pipeline definition. Immediately following is the =agent= directive, which specifies the execution environment for your pipeline stages. Commonly, you can use =any= (which will run on any available Jenkins agent, including the controller) or specify a Docker container. For instance, using a Docker image guarantees that your build stage runs in an environment with the required tools.

Within the pipeline, stages structure the workflow. Each =stage= block contains one or more =steps= that execute individual commands. These steps may run shell commands or invoke Jenkins Pipeline DSL commands. For more intricate logic beyond the declarative syntax, use the =script= step to incorporate complex Groovy code.

#+begin_src groovy
pipeline {
    agent any // Alternatively, you can use a Docker agent like: docker { image 'ubuntu:alpine' }
    stages {
        stage('Building') {
            agent { docker 'ubuntu:alpine' } // This stage runs within an Ubuntu Alpine Docker container
            steps {
                sh 'mvn clean package'
            }
        }
        stage('Unit Testing') {
            steps {
                script {
                    junit 'target/surefire-reports/*.xml'
                }
            }
        }
        stage('Deployment') {
            when {
                expression { branch == 'main' }
            }
            steps {
                sh 'deploy.sh'
            }
        }
    }
}
#+end_src

** Environment Variables

The environment directive lets you define key-value pairs for environment variables globally or within a specific stage. For example:

#+begin_src groovy
pipeline {
    environment {
        VAR1 = 'foo'
        VAR2 = 'bar'
    }
    stages {
        stage('Build') {
            steps {
                sh 'echo $VAR1' // Output: foo
            }
        }
        stage('Test') {
            environment { VAR1 = "test" }
            steps {
                sh 'echo $VAR1' // Output: test
            }
        }
    }
}
#+end_src

** Post Actions
The post directive specifies actions that run after the pipeline completes, whether it succeeds or fails. This feature is perfect for cleanup tasks or sending notifications.

#+begin_src groovy
pipeline {
    // ... pipeline stages and environment configuration
    post {
        success {
            script {
                // Send success notification
            }
        }
        failure {
            script {
                // Send failure notification
            }
        }
    }
}
#+end_src

A popular use case is integrating with plugins such as Slack Notifier:

#+begin_src groovy
pipeline {
    // ... pipeline stages
    post {
        always {
            slackNotifier(
                channel: '#your-slack-channel',
                message: "${currentBuild.result}"
            )
        }
    }
}
#+end_src

** Conditional Execution and Credentials

The when directive conditionally executes a stage based on factors such as branch name or build result:

#+begin_src groovy
pipeline {
    stages {
        stage('Deploy') {
            when {
                expression { branch == 'main' }
            }
            steps {
                sh 'deploy.sh'
            }
        }
    }
}
#+end_src

For handling secure credentials, use the =withCredentials= step:

#+begin_src groovy
pipeline {
    agent any
    stages {
        stage('Example') {
            steps {
                withCredentials([usernamePassword(credentialsId: 'myCredentials', usernameVariable: 'USERNAME', passwordVariable: 'PASSWORD')]) {
                    echo "Username: ${USERNAME}"
                }
            }
        }
    }
}
#+end_src

** Interactive Input and Parameters

During execution, the pipeline can prompt users using the =input= directive:

#+begin_src groovy
input {
    message 'Are you sure you want to deploy?'
    ok 'Yes'
    cancel 'No'
}
#+end_src

You can also define parameters to provide dynamic configuration when triggering a build:

#+begin_src groovy
pipeline {
    parameters {
        string(name: 'ENV_NAME', defaultValue: 'dev', description: 'Environment to deploy to')
    }
    environment {
        ENV = params.ENV_NAME
    }
    // ... pipeline stages
}
#+end_src

** Stashing and Unstashing Artifacts

With the stash and unstash directives, you can temporarily store build artifacts or files between stages or nodes. This method is especially useful for caching build results:

#+begin_src groovy
stage('Build') {
    steps {
        // ... build steps
        stash name: 'build-artifacts'
    }
}

stage('Deploy') {
    steps {
        unstash name: 'build-artifacts'
        // ... deployment steps using stashed artifacts
    }
}
#+end_src

** Parallel Execution

To optimize pipeline run times, stages can be executed concurrently using the parallel directive. Ensure that parallel stages are independent to avoid interfering with shared outputs:

#+begin_src groovy
pipeline {
    parallel {
        stage('Unit Testing') {
            steps {
                // ... unit test steps
            }
        }
        stage('Vulnerability Testing') {
            steps {
                // ... vulnerability test steps
            }
        }
    }
    // ... subsequent stages (optional)
}
#+end_src
